"""Performance evaluation utilities for crypto return models.

This script has two responsibilities:

1. Fit a simple linear regression on stacked embeddings vs. next-hour returns to
   build a reproducible baseline and output regression statistics.
2. Consume predictions generated by any downstream model (e.g., scripts/predict.py)
   to compute the metrics listed in to_do_list.txt, including a toy trading
   strategy that goes long/flat/short based on the predicted return.
"""

from __future__ import annotations

import argparse
import json
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    precision_recall_fscore_support,
    r2_score,
)


FEATURE_DIR = Path("output/features")
PRED_DIR = Path("output/predictions")
REPORT_DIR = Path("output/reports")
REPORT_DIR.mkdir(parents=True, exist_ok=True)

# Annualization factor for Sharpe ratio when working with hourly returns.
HOURS_PER_YEAR = 24 * 365


@dataclass
class RegressionMetrics:
    mse: float
    rmse: float
    mae: float
    mape_pct: float
    r2: float
    directional_accuracy: float
    pearson_ic: float
    spearman_ic: float
    precision_up: float
    recall_up: float
    precision_down: float
    recall_down: float


@dataclass
class StrategyMetrics:
    threshold: float
    avg_hourly_return: float
    cumulative_return: float
    sharpe: float
    hit_rate: float
    long_ratio: float
    short_ratio: float
    flat_ratio: float


def load_feature_arrays() -> Tuple[np.ndarray, np.ndarray]:
    X_path = FEATURE_DIR / "X.npy"
    y_path = FEATURE_DIR / "y.npy"

    if not X_path.exists() or not y_path.exists():
        raise FileNotFoundError(
            "Feature arrays not found. Did you run scripts/build_features.py?"
        )

    X = np.load(X_path)
    y = np.load(y_path)
    return X, y


def chronological_split(
    X: np.ndarray, y: np.ndarray, val_fraction: float
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    if not 0.0 < val_fraction < 1.0:
        raise ValueError("val_fraction must be in (0, 1)")

    split_idx = int(len(X) * (1.0 - val_fraction))
    return X[:split_idx], X[split_idx:], y[:split_idx], y[split_idx:]


def train_linear_baseline(
    X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray
) -> np.ndarray:
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model.predict(X_test)


def _safe_mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    eps = 1e-8
    denom = np.where(np.abs(y_true) < eps, eps, np.abs(y_true))
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0


def _directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    mask = (np.abs(y_true) > 1e-8) | (np.abs(y_pred) > 1e-8)
    if not np.any(mask):
        return float("nan")
    return np.mean(np.sign(y_true[mask]) == np.sign(y_pred[mask]))


def _classification_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, ...]:
    up_true = (y_true > 0).astype(int)
    up_pred = (y_pred > 0).astype(int)
    down_true = (y_true < 0).astype(int)
    down_pred = (y_pred < 0).astype(int)

    precision_up, recall_up, _, _ = precision_recall_fscore_support(
        up_true, up_pred, average="binary", zero_division=0
    )
    precision_down, recall_down, _, _ = precision_recall_fscore_support(
        down_true, down_pred, average="binary", zero_division=0
    )

    return precision_up, recall_up, precision_down, recall_down


def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> RegressionMetrics:
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = _safe_mape(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    dir_acc = _directional_accuracy(y_true, y_pred)

    pearson = float(np.corrcoef(y_true, y_pred)[0, 1])
    spearman = float(pd.Series(y_true).corr(pd.Series(y_pred), method="spearman"))

    precision_up, recall_up, precision_down, recall_down = _classification_metrics(
        y_true, y_pred
    )

    return RegressionMetrics(
        mse=mse,
        rmse=rmse,
        mae=mae,
        mape_pct=mape,
        r2=r2,
        directional_accuracy=dir_acc,
        pearson_ic=pearson,
        spearman_ic=spearman,
        precision_up=precision_up,
        recall_up=recall_up,
        precision_down=precision_down,
        recall_down=recall_down,
    )


def compute_signal_threshold(y_train: np.ndarray, percentile: float) -> float:
    if not 0 < percentile < 100:
        raise ValueError("percentile must be between 0 and 100")
    return float(np.percentile(np.abs(y_train), percentile))


def simulate_trading_strategy(
    y_true: np.ndarray, y_pred: np.ndarray, threshold: float
) -> StrategyMetrics:
    if threshold <= 0:
        raise ValueError("threshold must be positive")

    positions = np.where(
        y_pred >= threshold,
        1,
        np.where(y_pred <= -threshold, -1, 0),
    )

    returns = positions * y_true
    avg_hourly_return = float(np.mean(returns))
    cumulative_return = float(np.prod(1 + returns) - 1)
    sharpe = float(
        np.sqrt(HOURS_PER_YEAR) * avg_hourly_return / (np.std(returns) + 1e-8)
    )

    hit_rate = float(
        np.mean(np.sign(returns[positions != 0]) == np.sign(y_true[positions != 0]))
    ) if np.any(positions != 0) else float("nan")

    long_ratio = float(np.mean(positions == 1))
    short_ratio = float(np.mean(positions == -1))
    flat_ratio = float(np.mean(positions == 0))

    return StrategyMetrics(
        threshold=threshold,
        avg_hourly_return=avg_hourly_return,
        cumulative_return=cumulative_return,
        sharpe=sharpe,
        hit_rate=hit_rate,
        long_ratio=long_ratio,
        short_ratio=short_ratio,
        flat_ratio=flat_ratio,
    )


def evaluate_predictions(
    preds_path: Path,
    signal_threshold: float,
) -> Dict[str, Dict]:
    if not preds_path.exists():
        raise FileNotFoundError(f"Prediction file not found: {preds_path}")

    df = pd.read_csv(preds_path)
    if not {"pred", "target"} <= set(df.columns):
        raise ValueError("Prediction CSV must contain 'pred' and 'target' columns.")

    y_true = df["target"].to_numpy()
    y_pred = df["pred"].to_numpy()

    regression = compute_regression_metrics(y_true, y_pred)
    strategy = simulate_trading_strategy(y_true, y_pred, threshold=signal_threshold)

    return {
        "regression_metrics": asdict(regression),
        "strategy_metrics": asdict(strategy),
    }


def linear_regression_report(
    holdout_fraction: float,
    signal_percentile: float,
) -> Dict[str, Dict]:
    X, y = load_feature_arrays()
    X_train, X_test, y_train, y_test = chronological_split(X, y, holdout_fraction)

    preds = train_linear_baseline(X_train, y_train, X_test)

    regression = compute_regression_metrics(y_test, preds)
    threshold = compute_signal_threshold(y_train, signal_percentile)
    strategy = simulate_trading_strategy(y_test, preds, threshold)

    return {
        "regression_metrics": asdict(regression),
        "strategy_metrics": asdict(strategy),
        "threshold": threshold,
    }


def save_report(report: Dict, output_file: Path) -> None:
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with output_file.open("w") as f:
        json.dump(report, f, indent=2)
    print(f"Saved report â†’ {output_file}")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Generate performance report for baseline and model predictions."
    )
    parser.add_argument(
        "--predictions",
        type=Path,
        default=None,
        help="Optional CSV (with 'pred' and 'target') from scripts/predict.py.",
    )
    parser.add_argument(
        "--holdout",
        type=float,
        default=0.3,
        help="Fraction of the feature dataset reserved for testing the linear baseline.",
    )
    parser.add_argument(
        "--signal-percentile",
        type=float,
        default=60.0,
        help="Percentile of |return| used to set the long/short threshold.",
    )
    parser.add_argument(
        "--report-name",
        type=str,
        default="performance_report.json",
        help="Filename for the saved report inside output/reports/.",
    )

    args = parser.parse_args()

    print("\n=== Linear regression baseline ===")
    baseline = linear_regression_report(
        holdout_fraction=args.holdout,
        signal_percentile=args.signal_percentile,
    )

    if args.predictions is not None:
        print("\n=== Evaluating provided predictions ===")
        prediction_eval = evaluate_predictions(
            preds_path=args.predictions,
            signal_threshold=baseline["threshold"],
        )
    else:
        print("No prediction file supplied; skipping model comparison.")
        prediction_eval = {}

    report = {
        "linear_regression_baseline": baseline,
        "model_predictions": prediction_eval,
    }

    print(json.dumps(report, indent=2))
    save_report(report, REPORT_DIR / args.report_name)


if __name__ == "__main__":
    main()
