"""Performance evaluation utilities for crypto return models.

This script has two responsibilities:

1. Fit a simple linear regression on stacked embeddings vs. next-hour returns to
   build a reproducible baseline and output regression statistics.
2. Consume predictions generated by any downstream model (e.g., scripts/predict.py)
   to compute the metrics listed in to_do_list.txt, including a toy trading
   strategy that goes long/flat/short based on the predicted return.
"""

from __future__ import annotations

import argparse
import json
import matplotlib.pyplot as plt
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    precision_recall_fscore_support,
    r2_score,
)


FEATURE_DIR = Path("output/features")
PRED_DIR = Path("output/predictions")
REPORT_DIR = Path("output/reports")
REPORT_DIR.mkdir(parents=True, exist_ok=True)

CUTOFF_DEFAULT = "2024-10-01"

# Annualization factor for Sharpe ratio when working with hourly returns.
HOURS_PER_YEAR = 24 * 365


@dataclass
class RegressionMetrics:
    mse: float
    rmse: float
    mae: float
    mape_pct: float
    r2: float
    directional_accuracy: float
    pearson_ic: float
    spearman_ic: float
    precision_up: float
    recall_up: float
    precision_down: float
    recall_down: float


@dataclass
class StrategyMetrics:
    threshold: float
    avg_hourly_return: float
    cumulative_return: float
    sharpe: float
    hit_rate: float
    long_ratio: float
    short_ratio: float
    flat_ratio: float


def to_native_dict(mapping: Dict) -> Dict:
    """Convert numpy scalar values in a dict to native Python scalars."""
    native = {}
    for key, value in mapping.items():
        if isinstance(value, np.generic):
            native[key] = value.item()
        else:
            native[key] = value
    return native


def load_feature_arrays() -> Tuple[np.ndarray, np.ndarray]:
    X_path = FEATURE_DIR / "X.npy"
    y_path = FEATURE_DIR / "y.npy"

    if not X_path.exists() or not y_path.exists():
        raise FileNotFoundError(
            "Feature arrays not found. Did you run scripts/build_features.py?"
        )

    X = np.load(X_path)
    y = np.load(y_path)
    return X, y


def load_metadata_hours() -> np.ndarray:
    meta_path = FEATURE_DIR / "dataset.parquet"
    if not meta_path.exists():
        raise FileNotFoundError("dataset.parquet not found under output/features/")
    meta = pd.read_parquet(meta_path)
    hours = pd.to_datetime(meta["hour"]).dt.tz_localize(None).to_numpy()
    return hours


def chronological_split(
    X: np.ndarray, y: np.ndarray, val_fraction: float
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    if not 0.0 < val_fraction < 1.0:
        raise ValueError("val_fraction must be in (0, 1)")

    split_idx = int(len(X) * (1.0 - val_fraction))
    return X[:split_idx], X[split_idx:], y[:split_idx], y[split_idx:]


def train_linear_baseline(
    X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray
) -> np.ndarray:
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model.predict(X_test)


def _safe_mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    eps = 1e-8
    denom = np.where(np.abs(y_true) < eps, eps, np.abs(y_true))
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0


def _directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    mask = (np.abs(y_true) > 1e-8) | (np.abs(y_pred) > 1e-8)
    if not np.any(mask):
        return float("nan")
    return np.mean(np.sign(y_true[mask]) == np.sign(y_pred[mask]))


def _classification_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, ...]:
    up_true = (y_true > 0).astype(int)
    up_pred = (y_pred > 0).astype(int)
    down_true = (y_true < 0).astype(int)
    down_pred = (y_pred < 0).astype(int)

    precision_up, recall_up, _, _ = precision_recall_fscore_support(
        up_true, up_pred, average="binary", zero_division=0
    )
    precision_down, recall_down, _, _ = precision_recall_fscore_support(
        down_true, down_pred, average="binary", zero_division=0
    )

    return precision_up, recall_up, precision_down, recall_down


def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> RegressionMetrics:
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = _safe_mape(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    dir_acc = _directional_accuracy(y_true, y_pred)

    pearson = float(np.corrcoef(y_true, y_pred)[0, 1])
    spearman = float(pd.Series(y_true).corr(pd.Series(y_pred), method="spearman"))

    precision_up, recall_up, precision_down, recall_down = _classification_metrics(
        y_true, y_pred
    )

    return RegressionMetrics(
        mse=mse,
        rmse=rmse,
        mae=mae,
        mape_pct=mape,
        r2=r2,
        directional_accuracy=dir_acc,
        pearson_ic=pearson,
        spearman_ic=spearman,
        precision_up=precision_up,
        recall_up=recall_up,
        precision_down=precision_down,
        recall_down=recall_down,
    )


def compute_signal_threshold(y_train: np.ndarray, percentile: float) -> float:
    if not 0 < percentile < 100:
        raise ValueError("percentile must be between 0 and 100")
    return float(np.percentile(np.abs(y_train), percentile))


def simulate_trading_strategy(
    y_true: np.ndarray, y_pred: np.ndarray, threshold: float
) -> StrategyMetrics:
    if threshold <= 0:
        raise ValueError("threshold must be positive")

    positions = np.where(
        y_pred >= threshold,
        1,
        np.where(y_pred <= -threshold, -1, 0),
    )

    returns = positions * y_true
    avg_hourly_return = float(np.mean(returns))
    cumulative_return = float(np.prod(1 + returns) - 1)
    sharpe = float(
        np.sqrt(HOURS_PER_YEAR) * avg_hourly_return / (np.std(returns) + 1e-8)
    )

    hit_rate = float(
        np.mean(np.sign(returns[positions != 0]) == np.sign(y_true[positions != 0]))
    ) if np.any(positions != 0) else float("nan")

    long_ratio = float(np.mean(positions == 1))
    short_ratio = float(np.mean(positions == -1))
    flat_ratio = float(np.mean(positions == 0))

    return StrategyMetrics(
        threshold=threshold,
        avg_hourly_return=avg_hourly_return,
        cumulative_return=cumulative_return,
        sharpe=sharpe,
        hit_rate=hit_rate,
        long_ratio=long_ratio,
        short_ratio=short_ratio,
        flat_ratio=flat_ratio,
    )


def plot_strategy_curves(subset_name, positions, returns, timestamps, output_dir):
    if len(positions) == 0:
        return
    x = pd.to_datetime(timestamps)
    fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)
    axes[0].step(x, positions, where='post')
    axes[0].set_ylabel('Position')
    axes[0].set_title(f"{subset_name} positions")
    cum_returns = np.cumsum(returns)
    axes[1].plot(x, cum_returns)
    axes[1].set_ylabel('Cumulative Return')
    axes[1].set_xlabel('Time')
    axes[1].set_title(f"{subset_name} cumulative return")
    fig.autofmt_xdate()
    fig.tight_layout()
    fig_path = output_dir / f"strategy_{subset_name}.png"
    fig.savefig(fig_path)
    plt.close(fig)


def evaluate_predictions(
    preds_path: Path,
    signal_threshold: float,
    include_holdout: bool,
) -> Dict[str, Dict]:
    if not preds_path.exists():
        raise FileNotFoundError(f"Prediction file not found: {preds_path}")

    df = pd.read_csv(preds_path)
    if not {"pred", "target"} <= set(df.columns):
        raise ValueError("Prediction CSV must contain 'pred' and 'target' columns.")

    y_true = df["target"].to_numpy()
    y_pred = df["pred"].to_numpy()

    regression = compute_regression_metrics(y_true, y_pred)
    strategy = simulate_trading_strategy(y_true, y_pred, threshold=signal_threshold)
    
    subset_metrics = {}
    if "subset" in df.columns:
        subset_figs = []
        allowed_subsets = ["train", "test"]
        if include_holdout:
            allowed_subsets.append("holdout")
        for subset_name in sorted(df["subset"].dropna().unique()):
            if subset_name not in allowed_subsets:
                continue
            mask = df["subset"] == subset_name
            if not mask.any():
                continue
            reg = compute_regression_metrics(y_true[mask], y_pred[mask])
            strat = simulate_trading_strategy(y_true[mask], y_pred[mask], threshold=signal_threshold)
            positions = np.where(
                y_pred[mask] >= signal_threshold,
                1,
                np.where(y_pred[mask] <= -signal_threshold, -1, 0),
            )
            returns = positions * y_true[mask]
            timestamps = df.loc[mask, 'timestamp'].to_numpy() if 'timestamp' in df.columns else df.loc[mask].index.to_numpy()
            subset_figs.append((subset_name, positions, returns, timestamps))
            subset_metrics[subset_name] = {
                "regression_metrics": to_native_dict(asdict(reg)),
                "strategy_metrics": to_native_dict(asdict(strat)),
                "num_samples": int(mask.sum()),
            }
        for subset_name, positions, returns, timestamps in subset_figs:
            plot_strategy_curves(subset_name, positions, returns, timestamps, PRED_DIR)

    if subset_metrics:
        return {"subset_metrics": subset_metrics}

    return {
        "regression_metrics": to_native_dict(asdict(regression)),
        "strategy_metrics": to_native_dict(asdict(strategy)),
    }


def linear_regression_report(
    cutoff_date: str,
    pretest_fraction: float,
    signal_percentile: float,
    include_holdout: bool = False,
) -> Dict[str, Dict]:
    X, y = load_feature_arrays()
    hours = load_metadata_hours()
    if len(hours) != len(X):
        raise ValueError("dataset metadata rows do not match feature rows")

    cutoff_ts = pd.to_datetime(cutoff_date)
    pre_mask = hours < cutoff_ts
    holdout_mask = hours >= cutoff_ts
    if pre_mask.sum() < 2:
        raise ValueError("Not enough samples before cutoff to split train/test.")

    X_pre, y_pre = X[pre_mask], y[pre_mask]
    hours_pre = hours[pre_mask]
    X_holdout, y_holdout = X[holdout_mask], y[holdout_mask]
    holdout_hours = hours[holdout_mask]

    X_train, X_test, y_train, y_test = chronological_split(
        X_pre, y_pre, pretest_fraction
    )
    split_idx = len(X_train)
    test_hours = hours_pre[split_idx : split_idx + len(y_test)]

    preds = train_linear_baseline(X_train, y_train, X_test)

    regression = compute_regression_metrics(y_test, preds)
    threshold = compute_signal_threshold(y_train, signal_percentile)
    strategy = simulate_trading_strategy(y_test, preds, threshold)

    positions = np.where(
        preds >= threshold,
        1,
        np.where(preds <= -threshold, -1, 0),
    )
    returns = positions * y_test
    plot_strategy_curves("baseline_pretest", positions, returns, test_hours, REPORT_DIR)

    result = {
        "regression_metrics": to_native_dict(asdict(regression)),
        "strategy_metrics": to_native_dict(asdict(strategy)),
        "threshold": float(threshold),
    }

    if include_holdout and len(X_holdout) > 0:
        holdout_preds = train_linear_baseline(X_train, y_train, X_holdout)
        holdout_reg = compute_regression_metrics(y_holdout, holdout_preds)
        holdout_strat = simulate_trading_strategy(y_holdout, holdout_preds, threshold)
        holdout_positions = np.where(
            holdout_preds >= threshold,
            1,
            np.where(holdout_preds <= -threshold, -1, 0),
        )
        holdout_returns = holdout_positions * y_holdout
        plot_strategy_curves(
            "baseline_holdout",
            holdout_positions,
            holdout_returns,
            holdout_hours[: len(holdout_positions)],
            REPORT_DIR,
        )
        result["holdout_metrics"] = {
            "regression_metrics": to_native_dict(asdict(holdout_reg)),
            "strategy_metrics": to_native_dict(asdict(holdout_strat)),
        }

    return result


def save_report(report: Dict, output_file: Path) -> None:
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with output_file.open("w") as f:
        json.dump(report, f, indent=2)
    print(f"Saved report â†’ {output_file}")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Generate performance report for baseline and model predictions."
    )
    parser.add_argument(
        "--predictions",
        type=Path,
        default=None,
        help="Optional CSV (with 'pred' and 'target') from scripts/predict.py.",
    )
    parser.add_argument(
        "--cutoff-date",
        type=str,
        default=CUTOFF_DEFAULT,
        help="Date separating training from holdout evaluations (YYYY-MM-DD).",
    )
    parser.add_argument(
        "--pretest-fraction",
        type=float,
        default=0.2,
        help="Fraction of pre-cutoff data reserved for testing the baseline.",
    )
    parser.add_argument(
        "--include-holdout",
        action="store_true",
        help="Include holdout subset metrics when evaluating predictions/baseline.",
    )
    parser.add_argument(
        "--signal-percentile",
        type=float,
        default=50.0,
        help="Percentile of |return| used to set the long/short threshold.",
    )
    parser.add_argument(
        "--report-name",
        type=str,
        default="performance_report.json",
        help="Filename for the saved report inside output/reports/.",
    )

    args = parser.parse_args()

    print("\n=== Linear regression baseline ===")
    baseline = linear_regression_report(
        cutoff_date=args.cutoff_date,
        pretest_fraction=args.pretest_fraction,
        signal_percentile=args.signal_percentile,
        include_holdout=args.include_holdout,
    )

    if args.predictions is not None:
        print("\n=== Evaluating provided predictions ===")
        prediction_eval = evaluate_predictions(
            preds_path=args.predictions,
            signal_threshold=baseline["threshold"],
            include_holdout=args.include_holdout
        )
    else:
        print("No prediction file supplied; skipping model comparison.")
        prediction_eval = {}

    report = {
        "linear_regression_baseline": baseline,
        "model_predictions": prediction_eval,
    }

    print(json.dumps(report, indent=2))
    save_report(report, REPORT_DIR / args.report_name)


if __name__ == "__main__":
    main()
